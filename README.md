# ğŸµ MidiModel-Creator

**Create AI melody generators from your MIDI files!**

Transform your collection of MIDI files into a trained AI model that can generate new melodies in the same style. Perfect for musicians, composers, and AI enthusiasts who want to create personalized music generation models.

## âœ¨ What This Project Does

**MidiModel-Creator** is a complete pipeline that:

1. **ğŸ“ Processes MIDI Files** â†’ Converts your MIDI collection into AI-ready training data
2. **ğŸ¤– Trains AI Models** â†’ Creates transformer-based melody generation models  
3. **ğŸµ Generates New Music** â†’ Produces original melodies in your musical style

## ğŸ¯ Key Features

### ğŸ”§ **Dataset Creation**
- **Smart MIDI Processing**: Optimized for melody extraction from any MIDI file
- **Small File Support**: Works with short melodies (as few as 8 notes)
- **Quality Filtering**: Automatically removes poor-quality or corrupted files
- **Multi-instrument Handling**: Extracts melody lines from complex arrangements

### ğŸš€ **Advanced Training**
- **Beautiful CLI Interface**: Professional progress tracking and monitoring
- **GPU Optimized**: Automatic memory optimization for different GPU sizes
- **Real-time Monitoring**: Live loss/accuracy graphs and training statistics
- **Sample Generation**: Creates MIDI samples during training to monitor quality
- **Auto Checkpoints**: Never lose training progress with automatic saves

### âš™ï¸ **User-Friendly**
- **No Coding Required**: Simple setup and configuration files
- **Flexible Configuration**: YAML-based settings for easy customization
- **Global Ready**: Works on any Linux system with CUDA GPU

### ğŸµ **New! Orpheus CLI Interface**
- **Unified Experience**: Single entry point for all features
- **Smart Configuration**: Auto-detects GPU and suggests optimal settings
- **Live Dashboard**: Real-time training monitoring with progress bars
- **Beautiful UI**: Professional terminal interface with Rich library
- **Guided Workflow**: Step-by-step guidance through the entire process

## ğŸ“‹ Requirements

- **OS**: Linux (Ubuntu recommended)
- **GPU**: NVIDIA GPU with CUDA support (8GB+ VRAM recommended)
- **Python**: 3.8+
- **Storage**: ~5GB for dependencies + space for your MIDI files

## ğŸš€ Quick Start

### 1. **Setup**
```bash
git clone https://github.com/your-repo/MidiModel-Creator
cd MidiModel-Creator
bash setup.sh
pip install -r cli/requirements.txt  # Install CLI dependencies
```

### 2. **Run Unified CLI (Recommended)**
```bash
python app.py
```

This launches the **Orpheus Midi Model Maker** - a beautiful unified interface that guides you through:
- ğŸ¼ Creating datasets from MIDI files
- âœ… Validating your datasets
- ğŸš€ Training with auto-configured settings
- ğŸ“Š Live training dashboard with real-time metrics

### Alternative: Use Individual Scripts

#### Create Dataset
```bash
cd DatasetCreation
python start.py
# Choose "Create New Dataset"
# Provide path to your MIDI files
```

#### Train Model
```bash
cd ../TrainingModel
python app.py
# Provide path to your training config YAML
```

### 4. **Generate Music**
Your trained model will automatically generate sample MIDI files during training!

## ğŸ“‚ Project Structure

```
MidiModel-Creator/
â”œâ”€â”€ app.py                # New! Unified CLI entry point
â”œâ”€â”€ cli/                  # New! CLI interface modules
â”‚   â”œâ”€â”€ menu.py          # Main menu system
â”‚   â”œâ”€â”€ dataset_cli.py   # Dataset management
â”‚   â”œâ”€â”€ training_cli.py  # Training management
â”‚   â”œâ”€â”€ config_generator.py # Auto-configuration
â”‚   â”œâ”€â”€ dashboard.py     # Live training monitor
â”‚   â””â”€â”€ utils.py         # Shared utilities
â”œâ”€â”€ setup.sh              # One-time setup script
â”œâ”€â”€ DATA/                 # Processed datasets
â”œâ”€â”€ DatasetCreation/      # MIDI processing tools
â”‚   â”œâ”€â”€ start.py         # Main interface
â”‚   â”œâ”€â”€ processdata.py   # MIDI processor
â”‚   â””â”€â”€ verifydata.py    # Dataset validator
â””â”€â”€ TrainingModel/        # AI training module
    â”œâ”€â”€ app.py           # Training interface
    â”œâ”€â”€ config.py        # Configuration manager
    â”œâ”€â”€ trainer.py       # Training engine
    â””â”€â”€ sample_config.yml # Example settings
```

## ğŸµ What You Get

### **Input**: Your MIDI Files
- Classical compositions
- Jazz standards  
- Pop melodies
- Electronic music
- Any MIDI format

### **Output**: Trained AI Model
- Generates new melodies in your style
- Customizable length and creativity
- Professional MIDI output
- Ready for use in DAWs

## ğŸ”§ Configuration

Create a YAML config file to customize training:

```yaml
# Dataset path
dataset:
  path: "./DATA"

# Model size (adjust for your GPU)
model:
  seq_len: 2048    # Context length
  dim: 1280        # Model size
  depth: 8         # Transformer layers
  heads: 20        # Attention heads

# Training settings
training:
  batch_size: 8              # Batch size
  gradient_accumulate_every: 6
  learning_rate: 0.0001
  num_epochs: 5

# Output paths
output:
  model_dir: "./models"
  sample_dir: "./samples"
```

## ğŸ’¡ GPU Memory Guide

| GPU Memory | Recommended Settings |
|------------|---------------------|
| 8GB        | seq_len: 512, dim: 768, batch_size: 1 |
| 12GB       | seq_len: 1024, dim: 1024, batch_size: 2 |
| 16GB       | seq_len: 2048, dim: 1280, batch_size: 4 |
| 24GB+      | seq_len: 4096, dim: 1536, batch_size: 8 |

## ğŸ¯ Use Cases

- **ğŸ¼ Composers**: Generate melodic ideas and variations
- **ğŸ® Game Developers**: Create dynamic background music
- **ğŸ“š Music Students**: Analyze and learn from different musical styles  
- **ğŸ¤– AI Researchers**: Experiment with music generation models
- **ğŸµ Musicians**: Explore new creative possibilities

## ğŸš¨ What This Project Does NOT Do

- âŒ Generate full arrangements (drums, bass, harmony)
- âŒ Handle audio files (only MIDI)
- âŒ Real-time generation (training required first)
- âŒ Work without NVIDIA GPU

## ğŸ“Š Expected Results

### **Training Time**
- **Small dataset** (1,000 files): 2-4 hours
- **Medium dataset** (10,000 files): 8-12 hours  
- **Large dataset** (50,000+ files): 24-48 hours

### **Output Quality**
- **Beginner**: Recognizable melodies after 1 epoch
- **Good**: Musical coherence after 3 epochs
- **Excellent**: Style-accurate generation after 5 epochs

## ğŸ› ï¸ Troubleshooting

**Out of Memory?** â†’ Reduce `batch_size` and `seq_len` in config
**No MIDI files found?** â†’ Check file extensions (.mid, .midi)
**Poor quality output?** â†’ Increase training time or improve input data quality
**Training too slow?** â†’ Increase `batch_size` if you have more GPU memory

## ğŸ¤ Contributing

This project is built on:
- **[tegridy-tools](https://github.com/asigalov61/tegridy-tools)** - MIDI processing
- **[X-Transformer](https://github.com/lucidrains/x-transformers)** - Transformer architecture
- **PyTorch** - Deep learning framework

## ğŸ“„ License

[Add your chosen license here]

## ğŸµ Happy Music Making!

Transform your MIDI collection into an AI composer and discover new musical possibilities!

---
*Made with â¤ï¸ for the music and AI community*
